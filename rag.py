# rag.pyï¼ˆå«å®Œæ•´ LOG ç‰ˆ + æ”¯æ´èƒŒæ™¯åˆ·æ–°é‡æ–°è¼‰å…¥ï¼‰
import time
from retrievers.cache import (
    load_stock_map_from_cache,
    get_price_with_cache,
    get_news_with_cache,
)
from retrievers.news import fetch_news_rss
from retrievers.merge_utils import merge_news
from retrievers.fulltext import lazy_fulltext_topk
from urllib.parse import urlparse, urlunparse

def normalize_url(url: str) -> str:
    """æŠŠ URL æ­£è¦åŒ–ï¼šhttp->httpsã€ç§»é™¤ç©ºç™½ã€ä¿®æ­£ç‰¹å®šç¶²åŸŸã€è™•ç†è§£æå¤±æ•—"""
    if not url:
        return ""
    url = url.strip()

    # http -> httpsï¼ˆLINE/ç€è¦½å™¨é€šå¸¸æ›´ç©©ï¼‰
    if url.startswith("http://"):
        url = "https://" + url[len("http://"):]

    try:
        p = urlparse(url)
        if p.scheme not in ("http", "https") or not p.netloc:
            return ""

        host = (p.netloc or "").lower()

        # ä½ çš„ä¾‹å­ï¼šsinotrade.com.tw -> www.sinotrade.com.twï¼ˆå¯é¸ï¼‰
        if host == "sinotrade.com.tw":
            p = p._replace(netloc="www.sinotrade.com.tw")

        return urlunparse(p)
    except Exception:
        return ""


def looks_like_article(url: str) -> bool:
    """åˆ¤æ–·æ˜¯ä¸æ˜¯ã€æ–‡ç« é ã€è€Œä¸æ˜¯é¦–é ã€‚è¦å‰‡å¯å†èª¿ï¼Œä½†å…ˆç”¨é€™å€‹å°±å¾ˆæœ‰æ•ˆã€‚"""
    if not url:
        return False
    try:
        p = urlparse(url)
        # é¦–é ï¼ˆpath="" æˆ– "/"ï¼‰è¦–ç‚ºç„¡æ•ˆ
        if p.path in ("", "/") and not p.query:
            return False
        return True
    except Exception:
        return False


# === è‚¡ç¥¨ä»£è™Ÿå°ç…§è¡¨ ===
print("[RAG/Init] ğŸ§­ è¼‰å…¥ FinMind è‚¡ç¥¨ä»£è™Ÿæ¸…å–®ä¸­...")
STOCK_MAP = load_stock_map_from_cache()
print(f"[RAG/Init] âœ… å·²è¼‰å…¥ {len(STOCK_MAP)//2} æª”è‚¡ç¥¨ä»£è™Ÿã€‚")

# === ä½¿ç”¨è€…æŸ¥è©¢å¿«å– ===
CACHE = {}
CACHE_DURATION_SECONDS = 120


# ---------------------------------------------------------
# å…¬å¸è¾¨è­˜
# ---------------------------------------------------------
def smart_identify_company(query: str):
    q = query.strip().upper()
    print(f"[RAG/Identify] ğŸ” å˜—è©¦è¾¨è­˜å…¬å¸ï¼š'{q}'")

    # å®Œå…¨å‘½ä¸­ï¼ˆåç¨±æˆ–ä»£è™Ÿï¼‰
    if q in STOCK_MAP:
        print(f"[RAG/Identify] âœ… å®Œå…¨å‘½ä¸­ STOCK_MAP â†’ {q} â†’ {STOCK_MAP[q]}")
        return STOCK_MAP[q], q

    # æ¨¡ç³Šæ¯”å°
    for name in STOCK_MAP.keys():
        if len(name) >= 2 and name in q:
            print(f"[RAG/Identify] ğŸ” åµæ¸¬åˆ°å…¬å¸åç¨±ç‰‡æ®µ â†’ {name}")
            return STOCK_MAP[name], name

    print(f"[RAG/Identify] âŒ æ‰¾ä¸åˆ°åŒ¹é…çš„å…¬å¸ â†’ '{q}'")
    return None, None


# ---------------------------------------------------------
# ä¸»æµç¨‹ï¼šçµ„åˆ context
# ---------------------------------------------------------
def build_context(query: str):
    user_text = query.strip()
    now = time.time()
    print(f"[RAG/Query] ğŸš€ æ”¶åˆ°ä½¿ç”¨è€…æŸ¥è©¢ï¼šã€Œ{user_text}ã€")

    # --- å¿«å–æª¢æŸ¥ ---
    if user_text in CACHE and now < CACHE[user_text]['expires_at']:
        print(f"[RAG/Cache] âœ… ä½¿ç”¨å¿«å–è³‡æ–™ â†’ '{user_text}'ï¼ˆå‰©é¤˜ {int(CACHE[user_text]['expires_at'] - now)} ç§’ï¼‰")
        return CACHE[user_text]['data']
    print(f"[RAG/Cache] âŒ å¿«å–æœªå‘½ä¸­ï¼Œé–‹å§‹æŸ¥è©¢è³‡æ–™ â†’ '{user_text}'\n")

    # --- å…¬å¸è¾¨è­˜ ---
    ticker_id, company_name = smart_identify_company(user_text)
    if not ticker_id:
        print(f"[RAG/Query] âŒ æŸ¥ç„¡å…¬å¸ '{user_text}'ï¼Œçµ‚æ­¢æµç¨‹ã€‚")
        return f"æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°èˆ‡ã€Œ{user_text}ã€ç›¸é—œçš„å…¬å¸ï¼Œè«‹ç¢ºèªåç¨±æˆ–ä»£è™Ÿæ˜¯å¦æ­£ç¢ºã€‚"
    print(f"[RAG/Query] âœ… å…¬å¸è¾¨è­˜å®Œæˆï¼š{company_name}ï¼ˆä»£è™Ÿ {ticker_id}ï¼‰\n")

    # --- è‚¡åƒ¹æŸ¥è©¢ ---
    print(f"[RAG/Price] ğŸ’¹ é–‹å§‹æŸ¥è©¢è‚¡åƒ¹ â†’ {ticker_id}")
    price = get_price_with_cache(ticker_id)
    if price:
        print(f"[RAG/Price] âœ… è‚¡åƒ¹çµæœï¼š{price['price']} ({'+' if price['change']>=0 else ''}{price['change']}, {price['pct']}%)\n")
    else:
        print(f"[RAG/Price] âš ï¸ ç„¡æ³•å–å¾—è‚¡åƒ¹è³‡æ–™ã€‚")

    # --- æ–°èæŠ“å– ---
    print(f"[RAG/News] ğŸ—ï¸ é–‹å§‹æŠ“å–æ–°è â†’ FinMind + Google RSS")
    finmind_news = get_news_with_cache(ticker_id, company_name) or []
    rss_news = fetch_news_rss(company_name, ticker_id) or []

    # --- åˆä½µæ–°è ---
    print(f"[RAG/NewsMerge] ğŸ”„ æº–å‚™åˆä½µ FinMind èˆ‡ RSS æ–°è...")
    merged_news = merge_news(finmind_news, rss_news)
    print(f"[RAG/NewsMerge] âœ… åˆä½µå®Œæˆï¼Œå…± {len(merged_news)} å‰‡ã€‚\n")

    # --- çµ„è£ context ---
    print(f"[RAG/Context] ğŸ§© çµ„è£ context æ–‡å­—å…§å®¹...")
    ctx_lines = []

    if price:
        ctx_lines.append(f"[è‚¡åƒ¹è³‡è¨Š] {ticker_id} ç¾åƒ¹ {price['price']} ({'+' if price['change']>=0 else ''}{price['change']} / {price['pct']}%)")

    if merged_news:
        ctx_lines.append("[æ–°èä¾†æº (è«‹ç”¨ [ç·¨è™Ÿ] å¼•ç”¨)]")
        for i, n in enumerate(merged_news, start=1):
            title = (n.get("title") or "").strip()
            src = (n.get("source") or "").strip() or "æœªçŸ¥ä¾†æº"
            dt = (n.get("publishedAt") or "").strip() or "æœªçŸ¥æ—¥æœŸ"
            raw_url = (n.get("url") or "").strip()
            norm_url = normalize_url(raw_url)
            if not looks_like_article(norm_url):
                norm_url = ""
            # âœ… æŠŠæ­£è¦åŒ–å¾Œçš„çµæœå¯«å›å»ï¼Œè®“å¾Œé¢çš„ lazy full-text ç”¨å¾—åˆ°ä¹¾æ·¨ URL
            n["url"] = norm_url
            url = norm_url if norm_url else "ç„¡é€£çµ"

            # é€™è¡Œå°±æ˜¯ grounding çš„æ ¸å¿ƒï¼šLLM ä¹‹å¾Œå°±èƒ½ç”¨ [i]
            ctx_lines.append(f"[{i}] {title} | {src} | {dt} | {url}")

    # --- Lazy Full-Text Top3ï¼ˆåªæŠ“æœ€ç›¸é—œçš„ 3 ç¯‡å…¨æ–‡ï¼‰---
    ft_map = lazy_fulltext_topk(user_text, merged_news, k=3)
    if ft_map:
        ctx_lines.append("")
        ctx_lines.append("[å…¨æ–‡æ‘˜éŒ„ (Top3ï¼Œä»è«‹ç”¨ç›¸åŒ [ç·¨è™Ÿ] å¼•ç”¨)]")
        for idx in sorted(ft_map.keys()):
            for j, snippet in enumerate(ft_map[idx], start=1):
                ctx_lines.append(f"[{idx}] æ‘˜éŒ„{j}: {snippet}")

    if not ctx_lines:
        result = f"(æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°é—œæ–¼ã€Œ{user_text}ã€çš„å³æ™‚è³‡è¨Š)"
        print(f"[RAG/Context] âš ï¸ æœªå–å¾—ä»»ä½•è‚¡åƒ¹æˆ–æ–°èè³‡æ–™ã€‚")
    else:
        result = "\n".join(ctx_lines)
        print(f"[RAG/Context] âœ… çµ„è£å®Œæˆï¼Œå…± {len(merged_news)} å‰‡æ–°èã€‚\n")

    # --- å¯«å…¥å¿«å– ---
    CACHE[user_text] = {'data': result, 'expires_at': now + CACHE_DURATION_SECONDS}
    print(f"[RAG/Cache] ğŸ’¾ å·²å¿«å–çµæœ â†’ '{user_text}'ï¼ˆæœ‰æ•ˆ {CACHE_DURATION_SECONDS} ç§’ï¼‰")

    print(f"[RAG/Done] ğŸ æŸ¥è©¢æµç¨‹çµæŸï¼š'{user_text}'\n")
    #print(f"{result}\n")
    return result


# ---------------------------------------------------------
# èƒŒæ™¯åˆ·æ–°å¾Œé‡æ–°è¼‰å…¥è‚¡ç¥¨ä»£è™Ÿï¼ˆç”± cache.py å‘¼å«ï¼‰
# ---------------------------------------------------------
def refresh_stock_map():
    """ç”± cache.py èƒŒæ™¯æ›´æ–°å®Œæˆå¾Œå‘¼å«ï¼Œç”¨æ–¼é‡æ–°è¼‰å…¥æœ€æ–° STOCK_MAP"""
    print("[RAG/Init] ğŸ§­ è¼‰å…¥ FinMind è‚¡ç¥¨ä»£è™Ÿæ¸…å–®ä¸­ï¼ˆèƒŒæ™¯åˆ·æ–°å¾Œï¼‰...")
    from retrievers.cache import load_stock_map_from_cache
    global STOCK_MAP
    STOCK_MAP = load_stock_map_from_cache()
    print(f"[RAG/Init] âœ… å·²é‡æ–°è¼‰å…¥ {len(STOCK_MAP)//2} æª”è‚¡ç¥¨ä»£è™Ÿã€‚")
